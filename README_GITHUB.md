<div align="center">

# Web Crawler & Interactive Graph Generator

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/mehrabmahmoudifar/web-crawler-graph?style=social)](https://github.com/mehrabmahmoudifar/web-crawler-graph/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/mehrabmahmoudifar/web-crawler-graph?style=social)](https://github.com/mehrabmahmoudifar/web-crawler-graph/network/members)

**A powerful web crawler that maps website structure and generates beautiful interactive network visualizations.**

[Features](#features) â€¢ [Installation](#installation) â€¢ [Usage](#usage) â€¢ [Documentation](#documentation) â€¢ [Contributing](#contributing)

<img src="screenshots/demo.gif" alt="Demo" width="800">

[ğŸ‡¬ğŸ‡§ English](#english) | [ğŸ‡®ğŸ‡· ÙØ§Ø±Ø³ÛŒ](#persian)

</div>

---

## English

### Features

- ğŸš€ **Multi-threaded Crawling** - Fast concurrent crawling with configurable workers
- ğŸ¨ **Interactive Visualization** - Beautiful PyVis graphs with search and highlighting
- ğŸ” **Smart URL Handling** - Automatic normalization, deduplication, and validation
- ğŸ“Š **Data Export** - JSON export for further analysis
- ğŸ¯ **Domain Grouping** - Automatic categorization by subdomain
- ğŸ–±ï¸ **Interactive Navigation** - Click to highlight, double-click to open URLs
- âš¡ **Rate Limiting** - Respectful crawling with configurable delays
- ğŸ“ **Comprehensive Logging** - Detailed execution logs

### Quick Start

```bash
# Clone the repository
git clone https://github.com/mehrabmahmoudifar/web-crawler-graph.git
cd web-crawler-graph

# Install dependencies
pip install -r requirements.txt

# Run the crawler
python crawler.py https://example.com
```

### Output Files

| File | Description |
|------|-------------|
| `[domain]_interactive.html` | ğŸ¨ Interactive graph visualization |
| `[domain]_links.json` | ğŸ“Š Detailed link data |
| `[domain]_stats.json` | ğŸ“ˆ Basic statistics |
| `crawl_log.txt` | ğŸ“ Execution log |

### Interactive Graph Features

<table>
<tr>
<td width="50%">

**Search & Navigation**
- Find nodes by URL or label
- Zoom and pan controls
- Navigation buttons
- Keyboard shortcuts

</td>
<td width="50%">

**Visual Features**
- Color-coded by subdomain
- Hierarchical layout
- Smooth curved edges
- Hover tooltips

</td>
</tr>
<tr>
<td width="50%">

**Interactions**
- Click to highlight connections
- Double-click to open URLs
- Right-click for context menu
- Reset view button

</td>
<td width="50%">

**Data Display**
- In-degree indicators
- Out-degree indicators
- Connection highlighting
- Node grouping

</td>
</tr>
</table>

### Installation Options

#### Option 1: Basic Installation
```bash
git clone https://github.com/mehrabmahmoudifar/web-crawler-graph.git
cd web-crawler-graph
pip install -r requirements.txt
```

#### Option 2: Package Installation
```bash
pip install git+https://github.com/mehrabmahmoudifar/web-crawler-graph.git
```

#### Option 3: Development Mode
```bash
git clone https://github.com/mehrabmahmoudifar/web-crawler-graph.git
cd web-crawler-graph
pip install -e .
```

### Usage Examples

#### Basic Crawl
```python
from crawler import WebCrawler

crawler = WebCrawler("https://example.com")
graph = crawler.crawl()
print(f"Found {graph.number_of_nodes()} pages")
```

#### Custom Configuration
```python
from crawler import WebCrawler, StatsGenerator, InteractiveGraphGenerator

# High-performance crawling
crawler = WebCrawler(
    start_url="https://example.com",
    max_workers=10,
    rate_limit=0.5
)

graph = crawler.crawl()

# Generate visualization
stats_gen = StatsGenerator(graph, "https://example.com")
links_data = stats_gen.save_links("links.json")

visualizer = InteractiveGraphGenerator(links_data)
visualizer.generate("graph.html")
```

#### Command Line
```bash
# Basic usage
python crawler.py https://example.com

# Interactive mode
python crawler.py
```

### Requirements

- Python 3.8 or higher
- Internet connection
- Modern web browser (for viewing graphs)

### Dependencies

```
requests>=2.31.0
beautifulsoup4>=4.12.0
networkx>=3.1
matplotlib>=3.7.0
tqdm>=4.66.0
pyvis>=0.3.2
lxml>=4.9.0
```

### Documentation

- ğŸ“– [Full Documentation](README.md)
- ğŸ”§ [API Reference](API.md)
- ğŸ—ï¸ [Project Structure](PROJECT_STRUCTURE.md)
- ğŸš€ [Quick Start Guide](QUICK_START.md)
- ğŸ“ [Changelog](CHANGELOG.md)
- ğŸ¤ [Contributing Guidelines](CONTRIBUTING.md)

### Screenshots

<details>
<summary>Click to view screenshots</summary>

#### Interactive Graph
![Graph Example](screenshots/graph_example.png)

#### Search Feature
![Search Feature](screenshots/search_example.png)

#### Highlight Connections
![Highlight](screenshots/highlight_example.png)

</details>

### Performance

| Website Size | Nodes | Time | Memory |
|--------------|-------|------|---------|
| Small (<100 pages) | 50-100 | 1-2 min | 50-100 MB |
| Medium (100-1000 pages) | 100-500 | 5-15 min | 100-500 MB |
| Large (1000+ pages) | 500+ | 15+ min | 500 MB - 2 GB |

### Configuration

Modify crawler behavior:

```python
WebCrawler(
    start_url="https://example.com",
    max_workers=5,      # Number of concurrent threads
    rate_limit=1.0      # Minimum seconds between requests
)
```

### Node Color Groups

| Subdomain | Group | Color |
|-----------|-------|-------|
| mag.* | Mag | ğŸŸ£ Pink (#E91E63) |
| business.* | Business | ğŸŸ¢ Green (#4CAF50) |
| cloud.* | Cloud | ğŸ”µ Blue (#2196F3) |
| shop.* | Shop | ğŸŸ  Orange (#FF9800) |
| my.* | Panel | ğŸŸ£ Purple (#9C27B0) |
| (root) | Main | âš« Black (#212121) |

### Troubleshooting

<details>
<summary>Common Issues & Solutions</summary>

#### Issue: Too slow
**Solution:** Reduce `max_workers` or increase `rate_limit`
```python
crawler = WebCrawler(url, max_workers=3, rate_limit=2.0)
```

#### Issue: Memory error
**Solution:** Crawl specific sections or use more RAM

#### Issue: Access denied
**Solution:** Check website's `robots.txt` and respect their policies

#### Issue: ModuleNotFoundError
**Solution:** Install dependencies
```bash
pip install -r requirements.txt
```

</details>

### Roadmap

- [ ] robots.txt compliance
- [ ] Depth limit configuration
- [ ] Link validation (404 detection)
- [ ] Export to GEXF/GraphML
- [ ] Custom color schemes
- [ ] Content analysis
- [ ] Parallel domain crawling
- [ ] Web interface
- [ ] Docker support
- [ ] API endpoints

### Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for details.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

### Author

**Mehrab Mahmoudifar**

- GitHub: [@mehrabmahmoudifar](https://github.com/mehrabmahmoudifar)
- Email: mehrab.mahmoudifar@example.com

### Acknowledgments

Built with:
- [NetworkX](https://networkx.org/) - Graph processing
- [PyVis](https://pyvis.readthedocs.io/) - Interactive visualization
- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) - HTML parsing
- [Requests](https://requests.readthedocs.io/) - HTTP library

### Support

If you find this project helpful, please give it a â­ on GitHub!

<div align="center">

**[â¬† Back to top](#web-crawler--interactive-graph-generator)**

</div>

---

## Persian

<div dir="rtl" align="right">

### ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§

- ğŸš€ **Ø®Ø²Ø´ Ú†Ù†Ø¯-Ù†Ø®ÛŒ** - Ø®Ø²Ø´ Ø³Ø±ÛŒØ¹ Ù‡Ù…Ø²Ù…Ø§Ù† Ø¨Ø§ Ú©Ø§Ø±Ú¯Ø±Ù‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ ØªÙ†Ø¸ÛŒÙ…
- ğŸ¨ **Ù†Ù…Ø§ÛŒØ´ ØªØ¹Ø§Ù…Ù„ÛŒ** - Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ¨Ø§ÛŒ PyVis Ø¨Ø§ Ø¬Ø³ØªØ¬Ùˆ Ùˆ Ù‡Ø§ÛŒÙ„Ø§ÛŒØª
- ğŸ” **Ù…Ø¯ÛŒØ±ÛŒØª Ù‡ÙˆØ´Ù…Ù†Ø¯ URL** - Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒØŒ Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒ Ùˆ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±
- ğŸ“Š **Ø®Ø±ÙˆØ¬ÛŒ Ø¯Ø§Ø¯Ù‡** - Ø®Ø±ÙˆØ¬ÛŒ JSON Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¨ÛŒØ´ØªØ±
- ğŸ¯ **Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø§Ù…Ù†Ù‡** - Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø§Ø¨â€ŒØ¯Ø§Ù…ÛŒÙ†
- ğŸ–±ï¸ **Ù†Ø§ÙˆØ¨Ø±ÛŒ ØªØ¹Ø§Ù…Ù„ÛŒ** - Ú©Ù„ÛŒÚ© Ø¨Ø±Ø§ÛŒ Ù‡Ø§ÛŒÙ„Ø§ÛŒØªØŒ Ø¯Ø§Ø¨Ù„â€ŒÚ©Ù„ÛŒÚ© Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† URL
- âš¡ **Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø³Ø±Ø¹Øª** - Ø®Ø²Ø´ Ù…Ø­ØªØ±Ù…Ø§Ù†Ù‡ Ø¨Ø§ ØªØ£Ø®ÛŒØ±Ù‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ ØªÙ†Ø¸ÛŒÙ…
- ğŸ“ **Ù„Ø§Ú¯ Ø¬Ø§Ù…Ø¹** - Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚ Ø§Ø¬Ø±Ø§

### Ø´Ø±ÙˆØ¹ Ø³Ø±ÛŒØ¹

```bash
# Ú©Ù„ÙˆÙ† Ú©Ø±Ø¯Ù† Ù…Ø®Ø²Ù†
git clone https://github.com/mehrabmahmoudifar/web-crawler-graph.git
cd web-crawler-graph

# Ù†ØµØ¨ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§
pip install -r requirements.txt

# Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø±Ø§ÙˆÙ„Ø±
python crawler.py https://example.com
```

### ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ

| ÙØ§ÛŒÙ„ | ØªÙˆØ¶ÛŒØ­Ø§Øª |
|------|-------------|
| `[domain]_interactive.html` | ğŸ¨ Ù†Ù…Ø§ÛŒØ´ Ú¯Ø±Ø§Ù ØªØ¹Ø§Ù…Ù„ÛŒ |
| `[domain]_links.json` | ğŸ“Š Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚ Ù„ÛŒÙ†Ú© |
| `[domain]_stats.json` | ğŸ“ˆ Ø¢Ù…Ø§Ø± Ù¾Ø§ÛŒÙ‡ |
| `crawl_log.txt` | ğŸ“ Ù„Ø§Ú¯ Ø§Ø¬Ø±Ø§ |

### Ù…Ø³ØªÙ†Ø¯Ø§Øª

- ğŸ“– [Ù…Ø³ØªÙ†Ø¯Ø§Øª Ú©Ø§Ù…Ù„](README.md)
- ğŸ”§ [Ù…Ø±Ø¬Ø¹ API](API.md)
- ğŸ—ï¸ [Ø³Ø§Ø®ØªØ§Ø± Ù¾Ø±ÙˆÚ˜Ù‡](PROJECT_STRUCTURE.md)
- ğŸš€ [Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø´Ø±ÙˆØ¹ Ø³Ø±ÛŒØ¹](QUICK_START.md)
- ğŸ“ [ØªØºÛŒÛŒØ±Ø§Øª](CHANGELOG.md)
- ğŸ¤ [Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ù…Ø´Ø§Ø±Ú©Øª](CONTRIBUTING.md)

### Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ÛŒâ€ŒÙ‡Ø§

- Python 3.8 ÛŒØ§ Ø¨Ø§Ù„Ø§ØªØ±
- Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø§ÛŒÙ†ØªØ±Ù†Øª
- Ù…Ø±ÙˆØ±Ú¯Ø± ÙˆØ¨ Ù…Ø¯Ø±Ù† (Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§)

### Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡

**Ù…Ø­Ø±Ø§Ø¨ Ù…Ø­Ù…ÙˆØ¯ÛŒâ€ŒÙØ±**

- GitHub: [@astheysaymehrab](https://github.com/astheysaymehrab)
- Ø§ÛŒÙ…ÛŒÙ„: mehrabmahmoudifar98@gmail.com

### Ù…Ø¬ÙˆØ²

Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ ØªØ­Øª Ù…Ø¬ÙˆØ² MIT Ù…Ù†ØªØ´Ø± Ø´Ø¯Ù‡ Ø§Ø³Øª - ÙØ§ÛŒÙ„ [LICENSE](LICENSE) Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨Ø¨ÛŒÙ†ÛŒØ¯.

### Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ

Ø§Ú¯Ø± Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ Ø¨Ø±Ø§ÛŒ Ø´Ù…Ø§ Ù…ÙÛŒØ¯ Ø¨ÙˆØ¯ØŒ Ù„Ø·ÙØ§Ù‹ Ø¯Ø± GitHub ÛŒÚ© â­ Ø¨Ø¯Ù‡ÛŒØ¯!

</div>

<div align="center">

**Made with â¤ï¸ by Mehrab Mahmoudifar**

</div>
